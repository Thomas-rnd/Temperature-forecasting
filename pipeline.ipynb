{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.839602900Z",
     "start_time": "2023-11-28T15:23:45.888107900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.863122200Z",
     "start_time": "2023-11-28T15:23:49.846642Z"
    }
   },
   "id": "2f176304f9d29dd3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            data = pd.read_csv(self.file_path)\n",
    "            logging.info(\"Data loaded successfully.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            raise"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.865119Z",
     "start_time": "2023-11-28T15:23:49.856873800Z"
    }
   },
   "id": "2721344149d0f91d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Implement preprocessing steps like normalization, handling missing values etc.\n",
    "        # Example: self.data = (self.data - np.mean(self.data)) / np.std(self.data)\n",
    "        logging.info(\"Data preprocessing completed.\")\n",
    "        return self.data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.906566800Z",
     "start_time": "2023-11-28T15:23:49.870223500Z"
    }
   },
   "id": "7756be953e831a44"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.907075Z",
     "start_time": "2023-11-28T15:23:49.890711600Z"
    }
   },
   "id": "f301490b824c51bf"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    def build_model(self, input_size):\n",
    "        model = PyTorchModel(input_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        logging.info(\"PyTorch model built successfully.\")\n",
    "        return model, optimizer, criterion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.918253300Z",
     "start_time": "2023-11-28T15:23:49.901406300Z"
    }
   },
   "id": "a88e146d23f3e434"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, data, batch_size=32, epochs=10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Convert data to PyTorch tensors and create data loaders\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1).values, data['target'].values, test_size=0.2)\n",
    "        self.train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)), batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)), batch_size=self.batch_size)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            average_loss = running_loss / len(self.train_loader)\n",
    "            logging.info(f\"Epoch {epoch+1}/{self.epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                total_loss += loss.item()\n",
    "        average_loss = total_loss / len(self.test_loader)\n",
    "        logging.info(f\"Test Loss: {average_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:49.931741900Z",
     "start_time": "2023-11-28T15:23:49.920325600Z"
    }
   },
   "id": "f6e0c1ef75396798"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data loaded successfully.\n",
      "INFO:root:Data preprocessing completed.\n",
      "INFO:root:PyTorch model built successfully.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 39\u001B[0m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Visualization and report generation (optional)\u001B[39;00m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# These steps would require additional data or modifications\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# depending on your specific use case and available data\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;66;03m# report_generator = ReportGenerator(actual_values, predicted_values)\u001B[39;00m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# report_generator.generate_report()\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 39\u001B[0m     main()\n",
      "Cell \u001B[1;32mIn[21], line 17\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m builder \u001B[38;5;241m=\u001B[39m ModelBuilder()\n\u001B[0;32m     16\u001B[0m model, optimizer, criterion \u001B[38;5;241m=\u001B[39m builder\u001B[38;5;241m.\u001B[39mbuild_model(input_size)\n\u001B[1;32m---> 17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(model, optimizer, criterion, processed_data)\n\u001B[0;32m     18\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     19\u001B[0m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n",
      "Cell \u001B[1;32mIn[19], line 11\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, optimizer, criterion, data, batch_size, epochs)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Convert data to PyTorch tensors and create data loaders\u001B[39;00m\n\u001B[0;32m     10\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(data\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mvalues, data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loader \u001B[38;5;241m=\u001B[39m DataLoader(TensorDataset(torch\u001B[38;5;241m.\u001B[39mtensor(X_train, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32), torch\u001B[38;5;241m.\u001B[39mtensor(y_train, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loader \u001B[38;5;241m=\u001B[39m DataLoader(TensorDataset(torch\u001B[38;5;241m.\u001B[39mtensor(X_test, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32), torch\u001B[38;5;241m.\u001B[39mtensor(y_test, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size)\n",
      "\u001B[1;31mTypeError\u001B[0m: DataLoader.__init__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = 'example_temperature_data.csv'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    loader = DataLoader(file_path)\n",
    "    data = loader.load_data()\n",
    "    preprocessor = DataPreprocessor(data)\n",
    "    processed_data = preprocessor.preprocess()\n",
    "\n",
    "    # Assuming your data has features and a target column\n",
    "    # Adjust the input size to match the number of features in your dataset\n",
    "    input_size = processed_data.drop('target', axis=1).shape[1]\n",
    "\n",
    "    # Build and train the model\n",
    "    builder = ModelBuilder()\n",
    "    model, optimizer, criterion = builder.build_model(input_size)\n",
    "    trainer = Trainer(model, optimizer, criterion, processed_data)\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "    # Visualization and report generation (optional)\n",
    "    # These steps would require additional data or modifications\n",
    "    # depending on your specific use case and available data\n",
    "\n",
    "    # Example usage (modify as needed):\n",
    "    # visualizer = PredictionVisualizer(model, processed_data)\n",
    "    # future_data = ... # Load or create your future data for prediction\n",
    "    # visualizer.visualize_future_forecast(future_data)\n",
    "    # test_data = ... # Subset of processed_data or separate test data\n",
    "    # visualizer.compare_actual_vs_predicted(test_data)\n",
    "\n",
    "    # Generate report (modify as needed):\n",
    "    # actual_values = ... # Actual values from your dataset\n",
    "    # predicted_values = model.predict(...) # Predictions from your model\n",
    "    # report_generator = ReportGenerator(actual_values, predicted_values)\n",
    "    # report_generator.generate_report()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:26:47.776998600Z",
     "start_time": "2023-11-28T15:26:46.753284300Z"
    }
   },
   "id": "6449d568eeee74d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:23:50.148708500Z",
     "start_time": "2023-11-28T15:23:50.143511300Z"
    }
   },
   "id": "f3bd5898aadd81de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
