{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.303214300Z",
     "start_time": "2023-11-28T17:07:52.857182600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.311906100Z",
     "start_time": "2023-11-28T17:07:58.311393600Z"
    }
   },
   "id": "2f176304f9d29dd3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, file_path, test_size=0.2, val_size=0.1):\n",
    "        self.file_path = file_path\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "\n",
    "    def load_and_split_data(self):\n",
    "        try:\n",
    "            data = pd.read_csv(self.file_path)\n",
    "            # Splitting the data into train, validation, and test sets\n",
    "            train_data, test_data = train_test_split(data, test_size=self.test_size, random_state=42)\n",
    "            train_data, val_data = train_test_split(train_data, test_size=self.val_size / (1 - self.test_size), random_state=42)\n",
    "            logging.info(\"Data loaded and split successfully.\")\n",
    "            return train_data, val_data, test_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading and splitting data: {e}\")\n",
    "            raise"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.324941900Z",
     "start_time": "2023-11-28T17:07:58.318921700Z"
    }
   },
   "id": "2721344149d0f91d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Implement preprocessing steps like normalization, handling missing values etc.\n",
    "        # Example: self.data = (self.data - np.mean(self.data)) / np.std(self.data)\n",
    "        logging.info(\"Data preprocessing completed.\")\n",
    "        return self.data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.357028300Z",
     "start_time": "2023-11-28T17:07:58.324941900Z"
    }
   },
   "id": "7756be953e831a44"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.357028300Z",
     "start_time": "2023-11-28T17:07:58.335067100Z"
    }
   },
   "id": "f301490b824c51bf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    def build_model(self, input_size):\n",
    "        model = PyTorchModel(input_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        logging.info(f\"PyTorch model built successfully. Optimizer: {optimizer}, Loss: {criterion}\")\n",
    "        return model, optimizer, criterion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.357028300Z",
     "start_time": "2023-11-28T17:07:58.356518200Z"
    }
   },
   "id": "a88e146d23f3e434"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class CustomWeatherDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied on the target.\n",
    "        \"\"\"\n",
    "        self.weather_data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weather_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Assuming the last column is the target\n",
    "        features = self.weather_data.iloc[idx, :-1].values\n",
    "        target = self.weather_data.iloc[idx, -1]\n",
    "\n",
    "        # Convert to tensor\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return features, target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.370269700Z",
     "start_time": "2023-11-28T17:07:58.366536500Z"
    }
   },
   "id": "e2d85d58d8158325"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, train_csv, test_csv, batch_size=32, epochs=10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Load datasets\n",
    "        self.train_data = CustomWeatherDataset(csv_file=train_csv)\n",
    "        self.test_data = CustomWeatherDataset(csv_file=test_csv)\n",
    "\n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "\n",
    "        logging.info(\"Data loaders created successfully.\")\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for features, labels in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(features)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            average_loss = running_loss / len(self.train_loader)\n",
    "            logging.info(f\"Epoch {epoch+1}/{self.epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in self.test_loader:\n",
    "                outputs = self.model(features)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                total_loss += loss.item()\n",
    "        average_loss = total_loss / len(self.test_loader)\n",
    "        logging.info(f\"Test Loss: {average_loss:.4f}\")\n",
    "\n",
    "    def visualize_batch(self):\n",
    "        train_features, train_labels = next(iter(self.train_loader))\n",
    "        logging.info(f\"Feature batch shape: {train_features.size()}\")\n",
    "        logging.info(f\"Labels batch shape: {train_labels.size()}\")\n",
    "        example_features = train_features[0]\n",
    "        example_label = train_labels[0]\n",
    "        plt.plot(example_features.numpy(), label='Features')\n",
    "        plt.title(f'Example Weather Data Features with Label: {example_label.item()}')\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Feature Value')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.414405800Z",
     "start_time": "2023-11-28T17:07:58.382277700Z"
    }
   },
   "id": "f6e0c1ef75396798"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class PredictionVisualizer:\n",
    "    def __init__(self, model, data):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "\n",
    "    def visualize_future_forecast(self, future_data):\n",
    "        predictions = self.model.predict(future_data)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions)\n",
    "        plt.title(\"Future Temperature Forecast\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Temperature\")\n",
    "        plt.savefig(\"future_forecast.png\")\n",
    "        plt.show()\n",
    "\n",
    "    def compare_actual_vs_predicted(self, test_data):\n",
    "        actual = test_data['target']\n",
    "        predicted = self.model.predict(test_data.drop('target', axis=1))\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(actual, label='Actual')\n",
    "        plt.plot(predicted, label='Predicted')\n",
    "        plt.title(\"Comparison of Actual vs Predicted Temperatures\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Temperature\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"actual_vs_predicted.png\")\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.414405800Z",
     "start_time": "2023-11-28T17:07:58.398393500Z"
    }
   },
   "id": "171982e690287aba"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    def __init__(self, actual, predicted):\n",
    "        self.actual = actual\n",
    "        self.predicted = predicted\n",
    "\n",
    "    def generate_report(self):\n",
    "        report_df = pd.DataFrame({\n",
    "            'Actual': self.actual,\n",
    "            'Predicted': self.predicted.squeeze(),\n",
    "            'Difference': self.actual - self.predicted.squeeze()\n",
    "        })\n",
    "        report_df['Error'] = report_df['Difference'].abs()\n",
    "        report_df['Squared Error'] = report_df['Error'] ** 2\n",
    "\n",
    "        # Additional statistics\n",
    "        report_df['Mean Actual'] = self.actual.mean()\n",
    "        report_df['Mean Predicted'] = self.predicted.mean()\n",
    "        report_df['Standard Deviation Actual'] = self.actual.std()\n",
    "        report_df['Standard Deviation Predicted'] = self.predicted.std()\n",
    "\n",
    "        report_df.to_csv('report.csv', index=False)\n",
    "        logging.info(\"Report generated and saved as report.csv.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:58.414405800Z",
     "start_time": "2023-11-28T17:07:58.407452600Z"
    }
   },
   "id": "b8aa38d7d4259fa0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data loaded and split successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 40\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# Visualization and report generation (optional)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# These steps would require additional data or modifications\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# depending on your specific use case and available data\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# report_generator = ReportGenerator(actual_values, predicted_values)\u001B[39;00m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;66;03m# report_generator.generate_report()\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 40\u001B[0m     main()\n",
      "Cell \u001B[1;32mIn[11], line 9\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m train_data, val_data, test_data \u001B[38;5;241m=\u001B[39m data_loader\u001B[38;5;241m.\u001B[39mload_and_split_data()\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Preprocess data\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m preprocessor \u001B[38;5;241m=\u001B[39m DataPreprocessor(data)\n\u001B[0;32m     10\u001B[0m processed_data \u001B[38;5;241m=\u001B[39m preprocessor\u001B[38;5;241m.\u001B[39mpreprocess()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Assuming your data has features and a target column\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = 'example_temperature_data.csv'\n",
    "    \n",
    "    # Initialize DataLoader and split data\n",
    "    data_loader = DataLoader(file_path, test_size=0.2, val_size=0.1)\n",
    "    train_data, val_data, test_data = data_loader.load_and_split_data()\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor = DataPreprocessor(train_data)\n",
    "    processed_data = preprocessor.preprocess()\n",
    "\n",
    "    # Assuming your data has features and a target column\n",
    "    input_size = train_data.drop('target', axis=1).shape[1]  # Adjust 'target' as per your dataset\n",
    "\n",
    "    # Build and train the model\n",
    "    builder = ModelBuilder()\n",
    "    model, optimizer, criterion = builder.build_model(input_size)\n",
    "    trainer = Trainer(model, optimizer, criterion, train_data, val_data, test_data, batch_size=32, epochs=10)\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "    # Visualization and report generation (optional)\n",
    "    # These steps would require additional data or modifications\n",
    "    # depending on your specific use case and available data\n",
    "\n",
    "    # Example usage (modify as needed):\n",
    "    # visualizer = PredictionVisualizer(model, processed_data)\n",
    "    # future_data = ... # Load or create your future data for prediction\n",
    "    # visualizer.visualize_future_forecast(future_data)\n",
    "    # test_data = ... # Subset of processed_data or separate test data\n",
    "    # visualizer.compare_actual_vs_predicted(test_data)\n",
    "\n",
    "    # Generate report (modify as needed):\n",
    "    # actual_values = ... # Actual values from your dataset\n",
    "    # predicted_values = model.predict(...) # Predictions from your model\n",
    "    # report_generator = ReportGenerator(actual_values, predicted_values)\n",
    "    # report_generator.generate_report()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:07:59.235466500Z",
     "start_time": "2023-11-28T17:07:58.414405800Z"
    }
   },
   "id": "6449d568eeee74d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-28T17:07:59.235466500Z"
    }
   },
   "id": "f3bd5898aadd81de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
